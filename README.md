# Interpretable Mixed-type Mixture Modeling (IM3)

![Repo Name](https://img.shields.io/badge/Repo-interpretable--mixed--mixture-blueviolet)
![Status](https://img.shields.io/badge/Status-Research%20Proposal-blue)
![Python](https://img.shields.io/badge/Python-3.9%2B-green)
![Topic](https://img.shields.io/badge/Topic-Mixture%20Models%20%7C%20NLP%20%7C%20XAI-orange)

## ðŸ“Œ Overview

This project proposes a novel statistical framework for clustering **mixed-type data** composed of categorical variables (e.g., demographics) and high-dimensional text data. 

Unlike traditional approaches that rely on Bag-of-Words (BoW) and strong independence assumptions (Naive Bayes), this study leverages **Large Language Model (LLM) embeddings** to capture semantic context. To address the "Black-box" nature of embeddings and the "Curse of dimensionality," we introduce a **De-embedding** strategy for model interpretation and a dimensionality reduction step for stable Gaussian Mixture modeling.

## ðŸ“– Motivation

### The Limitation of Existing Methods
Recent studies, such as [Shi et al. (2024)](https://doi.org/10.1214/24-AOAS1893), utilize **Mixture Conditional Regression (MCR)** to estimate extralegal factors in judicial decisions. However, these methods typically model text data $Z$ as a binary vector under a **Naive Bayes assumption**:

$$
P(Z_i | K_i = k) = \prod_{j=1}^{p} P(Z_{ij} | K_i = k)
$$

**Critique:**
1.  **Loss of Semantics:** Binary indicators ignore word order, context, and semantic nuance.
2.  **Unrealistic Assumption:** The independence assumption between words is often violated in natural language.

### Our Approach
We propose replacing the binary feature vector with **dense embeddings** generated by LLMs (e.g., SBERT, OpenAI). This transforms the problem into a **Mixed-type Mixture Model** (Categorical + Continuous). To ensure interpretability and computational feasibility, we integrate dimensionality reduction and a post-hoc "De-embedding" analysis.

## ðŸ› ï¸ Methodology

The proposed framework consists of three main stages:

```mermaid
graph LR
    A[Raw Data] --> B{Preprocessing};
    B -->|Categorical X_cat| C[One-Hot Encoding];
    B -->|Text Z| D[LLM Embedding];
    D -->|High-dim Vector| E["Dim Reduction (PCA/UMAP)"];
    E -->|"Reduced Vector X_cont"| F[Joint Mixture Modeling];
    C --> F;
    F --> G[Latent Class Identification];
    G --> H[Interpretation via De-embedding];
```

1. Feature RepresentationLet the observed data for subject $i$ be $(\mathbf{x}^{(c)}_i, \mathbf{z}_i)$, where $\mathbf{x}^{(c)}_i$ is the categorical vector and $\mathbf{z}_i$ is the raw text.Text Embedding: $\mathbf{v}_i = \text{LLM}(\mathbf{z}_i) \in \mathbb{R}^{D}$ (where $D$ is large, e.g., 768 or 1536).Dimensionality Reduction: To ensure stable covariance estimation in GMM, we project $\mathbf{v}_i$ to a lower-dimensional space $\mathbb{R}^{d}$ (e.g., $d \approx 50$):$$\mathbf{x}^{(e)}_i = \phi(\mathbf{v}_i) \in \mathbb{R}^d$$where $\phi$ denotes a mapping function such as Principal Component Analysis (PCA).2. Joint Mixture Model SpecificationWe assume the population consists of $K$ latent classes. The joint likelihood for the mixed-type data $(\mathbf{x}^{(c)}_i, \mathbf{x}^{(e)}_i)$ is defined as:$$\mathcal{L}(\Theta) = \sum_{i=1}^{n} \log \left( \sum_{k=1}^{K} \pi_k \cdot f_{\text{cat}}(\mathbf{x}^{(c)}_i | \boldsymbol{\alpha}_k) \cdot f_{\text{cont}}(\mathbf{x}^{(e)}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right)$$Where:Mixing Proportion: $\pi_k$ satisfies $\sum \pi_k = 1$.Categorical Part: $f_{\text{cat}}$ follows a Multinomial distribution parameterized by $\boldsymbol{\alpha}_k$.Continuous (Embedding) Part: $f_{\text{cont}}$ follows a Multivariate Gaussian distribution:$$f_{\text{cont}}(\mathbf{x}^{(e)}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) = (2\pi)^{-d/2}|\boldsymbol{\Sigma}_k|^{-1/2} \exp\left(-\frac{1}{2}(\mathbf{x}^{(e)}_i - \boldsymbol{\mu}_k)^T \boldsymbol{\Sigma}_k^{-1} (\mathbf{x}^{(e)}_i - \boldsymbol{\mu}_k)\right)$$3. Interpretation Strategy (De-embedding)Since $\boldsymbol{\mu}_k$ lies in the embedding space, it is not directly interpretable. We propose two "De-embedding" methods to recover the semantic meaning of each latent class $k$.Method A: Semantic Anchor (Retrieval-based)We identify Prototype Documents that are closest to the cluster centroid $\boldsymbol{\mu}_k$.$$\text{Prototype}_k = \{ \mathbf{z}_j \mid \mathbf{z}_j \in \text{Dataset}, \text{argmax}_{j} \text{CosineSim}(\mathbf{x}^{(e)}_j, \boldsymbol{\mu}_k) \}$$This allows for qualitative analysis by reading the actual texts representing the cluster.Method B: Linear Decoder (Keyword Extraction)We train a linear decoder (or Lasso regression) to map embeddings back to the Bag-of-Words (BoW) space to find representative keywords.$$\hat{\mathbf{W}} = \underset{\mathbf{W}}{\text{argmin}} \sum_{i=1}^n || \mathbf{y}_{\text{BoW}, i} - \mathbf{x}^{(e)}_i \mathbf{W} ||_2^2 + \lambda ||\mathbf{W}||_1$$Using $\hat{\mathbf{W}}$, we transform the centroid $\boldsymbol{\mu}_k$ to a keyword vector $\mathbf{w}_k = \boldsymbol{\mu}_k \hat{\mathbf{W}}$, extracting the top-weighted words.ðŸš€ Key ContributionsSemantic-Aware Clustering: Overcomes the limitations of Naive Bayes by utilizing context-rich LLM embeddings.Unified Framework: Provides a statistical model for jointly analyzing demographics and unstructured text.Explainability: Bridges the gap between "Black-box" embeddings and "White-box" statistical inference via the proposed de-embedding strategies.ðŸ“‚ Project Structure (Tentative).
â”œâ”€â”€ data/                  # Raw and processed datasets
â”œâ”€â”€ notebooks/             # Jupyter notebooks for experimentation
â”‚   â”œâ”€â”€ 01_embedding.ipynb # LLM Embedding generation
â”‚   â”œâ”€â”€ 02_dim_reduction.ipynb
â”‚   â”œâ”€â”€ 03_mixture_model.ipynb
â”‚   â””â”€â”€ 04_de_embedding.ipynb
â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ models/            # GMM and Mixed-mixture implementations
â”‚   â””â”€â”€ utils/             # Helper functions for metrics
â”œâ”€â”€ README.md              # Project overview
â””â”€â”€ requirements.txt       # Dependencies
ðŸ“š ReferencesPrimary Reference: Shi, J., Wang, F., Gao, Y., Song, X., & Wang, H. (2024). Mixture conditional regression for estimating extralegal factor effects. The Annals of Applied Statistics, 18(3), 2535-2550.Mixture Models: Scrucca, L., Fop, M., Murphy, T. B., & Raftery, A. E. (2016). mclust 5: clustering, classification and density estimation using Gaussian finite mixture models. The R Journal, 8(1), 289.Embeddings: Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.
