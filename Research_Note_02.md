
## 1. 연구 배경 및 지난 미팅 요약 (Recap)

- **기존 제안:** 범주형 변수와 텍스트(LLM 임베딩)를 결합한 혼합 모형(Joint Mixture Model) 개발.
    
- **문제점(Pain Point):** 고차원 임베딩을 PCA로 축소하여 GMM에 넣을 경우, 추출된 모든 주성분(PC)이 군집화에 참여하게 됨. 이로 인해 **"어떤 의미적 차원이 군집을 나누는가?"**에 대한 명확한 해석(Interpretation)이 어려움.
    
- **교수님 피드백:** 이질적인 모집단 모델링에서 변수 선택과 이질성 탐색을 동시에 수행하는 **Li et al. (2022)** 방법론 검토 제안.
    

---

## 2. 방법론 업그레이드 전략: "PCA + Heterogeneity Pursuit"

고차원 LLM 임베딩($p \approx 768$)의 특성과 Li et al. 방법론의 장점을 결합하기 위해 다음의 2단계 프로세스를 제안합니다. 
- Li et al. 방법론: 단순히 데이터를 군집화하는 것을 넘어, **군집 간의 차이를 유발하는 '진짜 원인(Source of Heterogeneity)' 변수만 선별**하고, 나머지 군집 간 차이가 없는 변수는 공통 효과(Common Effect)로 처리하여 모델에서 배제합니다.


### Step 1: 차원 축소 및 직교화 (Dimensionality Reduction via PCA)

- **수행:** 고차원 텍스트 임베딩 $\mathbf{v}_i$ ($768$-dim)에 PCA를 적용하여 저차원 벡터 $\mathbf{x}^{(e)}_i$ ($d \approx 30$-dim) 생성.
    
- **목적:**
    
    1. **추정 안정성 확보:** 혼합 모형(Mixture Model)에서의 파라미터 폭발 문제 방지.
        
    2. **노이즈 제거:** 임베딩 내의 불필요한 정보(Noise)를 하위 주성분 제거를 통해 1차 필터링.
        

### Step 2: 이질성 추적 (Heterogeneity Pursuit via Li et al.)

- **수행:** 축소된 $d$개의 주성분(PC)을 입력으로 하여, **정규화된 혼합 효과 모형(Regularized Mixture Effects Model)** 적합1.
    
- 모델 구조 (Reparameterization):
    
    각 군집 $k$의 평균 벡터 $\boldsymbol{\mu}_k$를 공통 효과와 군집별 편차로 분해2:
    
    $$\boldsymbol{\mu}_{k} = \boldsymbol{\beta}_{0} + \boldsymbol{\beta}_{k}, \quad \text{subject to } \sum_{k=1}^{K} \boldsymbol{\beta}_{k} = 0$$
    
- 변수 선택 (Penalty):
    
    $\boldsymbol{\beta}_{k}$에 Adaptive Lasso ($l_1$) 페널티를 적용하여, 군집 간 차이가 없는 차원의 계수를 0으로 축소3.

---

## 3. 기존 방식 vs. 제안 방식 비교 (Why this is better?)

추천해주신 방법론을 적용했을 때, **'해석력(Interpretability)'** 측면에서 다음과 같은 명확한 이점이 있습니다.

| **구분**     | **Option A: 기존 PCA + GMM**                                | **Option B: PCA + Li et al. (Mix-HP)**                                    |
| ---------- | --------------------------------------------------------- | ------------------------------------------------------------------------- |
| **변수 활용**  | $d$개의 모든 주성분을 군집화에 사용                                     | **군집을 구분하는 핵심 차원만 선별** (Feature Selection)                                |
| **해석의 결**  | **포괄적(Holistic) 해석**<br><br>_"모든 차원이 복합적으로 작용하여 A군집입니다."_ | **판별적(Discriminative) 해석**<br><br>_"PC3(경쟁)과 PC7(보상)만이 이 군집을 만드는 원인입니다."_ |
| **공통 정보**  | 도메인 특유의 불용어(Stopwords)나 공통 문맥이 군집 결과에 노이즈로 작용             | 공통 효과($\beta_0$)로 흡수되어 **군집화 과정에서 제거됨**                                   |
| **모델 복잡도** | 높음 (파라미터 수 많음)                                            | **낮음 (희소성/Sparsity 확보)**                                                  |

### [예시 시나리오: 게임 마케팅 데이터]

- **PC1 (분산 최대):** '재미', '모바일', '출시' (모든 광고에 공통 등장)
    
- **PC3 (분산 중간):** '경쟁', '랭킹', 'PVP' (하드코어 유저 타겟)
    

> Option A 결과: "Cluster 1은 PC1도 높고 PC3도 높습니다." (특징이 흐릿함)
> 
> **Option B 결과:** "PC1은 공통 효과로 제거되었습니다. Cluster 1은 오직 **PC3(경쟁)**에 의해 정의되는 집단입니다."

**왜 "PCA 후 Li et al."인가?**
Li et al. 방법론을 원본 임베딩에 직접 적용하지 않고, **PCA를 선행하는 이유**에 대한 통계적/실용적 타당성입니다.

| **비교 항목**  | **Method A: Li et al. Only (원본 적용)**                                                                      | **Method B: PCA + Li et al. (제안)**                                                                       |
| ---------- | --------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- |
| **차원의 문제** | **$p=768 \gg$ Simulation Range**<br><br>  <br><br>논문은 $p=120$까지만 검증함4. $n$이 충분히 크지 않으면 수렴 실패 및 과적합 위험 높음. | **$p \approx 30 \in$ Stable Range**<br><br>  <br><br>논문의 시뮬레이션 범위 내로 차원을 통제하여, 추정의 일치성(Consistency) 보장5. |
| **변수의 의미** | **해석 불가 (Uninterpretable)**<br><br>  <br><br>Dense Vector의 135번째 차원이 선택되어도, 이것이 어떤 의미인지 해석할 방법이 없음.       | **해석 가능 (Interpretable)**<br><br>  <br><br>"PC3(경쟁 요소)"와 같이 주성분 단위의 의미 부여가 가능하여, 연구 목적에 부합함.             |
| **계산 비용**  | **매우 높음 (Infeasible)**<br>수십만 개의 공분산 파라미터 추정 필요.                                                          | **효율적 (Efficient)**<br>현실적인 시간 내에 최적의 $\lambda$ 튜닝 가능.                                                   |

---

## 4. 기대 효과 및 향후 계획

### 기대 효과

1. **통계적 엄밀성:** 단순히 "해석 가능하다"고 주장하는 것을 넘어, **'Scaled Source of Heterogeneity'** 6라는 통계적 정의를 통해 군집의 특징을 엄밀하게 규명할 수 있습니다.
    
2. **노이즈 강건성:** LLM 임베딩에 포함된 불필요한 의미 정보(Noise)를 벌점화(Regularization)를 통해 효과적으로 필터링할 수 있습니다.
    
### 시나리오 예시 (게임 마케팅 데이터)

- **PC1 (최대 분산):** '재미', '다운로드', '모바일' (모든 광고에 공통 등장 $\rightarrow$ 공통 효과)
    
- **PC2 (차순위 분산):** '경쟁', 'PVP', '랭킹' (하드코어 유저 타겟 $\rightarrow$ 이질성 원인)
    

#### [기존 방식: PCA + GMM]

- **결과:** "Cluster A는 PC1도 높고 PC2도 높습니다."
    
- **한계:** PC1(공통 단어)의 영향력이 커서, 군집 간의 본질적인 차이가 희석됨. **(Holistic View)**
    

#### [제안 방식: PCA + Mix-HP]

- **결과:** "PC1은 모든 군집에서 동일하므로 제거되었습니다($\hat{\beta}_{k,1} \to 0$). Cluster A를 정의하는 것은 오직 **PC2(경쟁)**입니다."
    
- **성과:** 군집을 구분 짓는 '판별적 차원(Discriminative Dimensions)'만을 선별하여 해석의 선명성 확보. (Discriminative View)

---
### 향후 계획 (Next Step)

1. Li et al. (2022) 저자가 제공한 R 패키지(`Mix-HP`) 분석 및 Python 환경으로의 포팅 가능성 검토7.
    
2. 시뮬레이션 데이터(Regime A/B)에 Mix-HP 알고리즘을 적용하여, 기존 GMM 대비 **변수 선택 정확도(TPR/FPR)** 비교 검증8.
    

---

## 5. 교수님께 드릴 질문 (Q&A)

1. **차원 축소 단계:** Li et al. 논문은 $p=120$ 수준까지 시뮬레이션 되었는데9, LLM 임베딩($p=768$)을 바로 넣는 것보다는 PCA로 전처리 후 적용하는 현재의 제 '하이브리드' 전략이 통계적으로 타당하다고 보시는지 의견을 여쭙고 싶습니다.
    
2. **페널티 선정:** 논문에서는 $L_1$ 페널티(Lasso)를 주로 사용했으나 10, 임베딩 차원 간의 상관성을 고려할 때 Elastic Net이나 Group Lasso 등으로 확장하는 것에 대해 어떻게 생각하시는지요?
